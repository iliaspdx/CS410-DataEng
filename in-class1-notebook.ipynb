{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# %matplotlib inline\n",
    " \n",
    "# Also import the libraries shown below. You will use the urllib.request module to open URLs. And youâ€™ll use  BeautifulSoup to extract data from html files. The Beautiful Soup library's name is bs4 which stands for BeautifulSoup, version 4.\n",
    "\n",
    "# from urllib.request import urlopen\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# After importing necessary modules, you should specify the URL containing the dataset and pass it to urlopen() to get the html of the page.\n",
    "\n",
    "# url = \"http://www.hubertiming.com/results/2017GPTR10K\"\n",
    "# html = urlopen(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mailto:timing@hubertiming.com\n",
      "https://www.hubertiming.com\n",
      "/results/2017GPTR\n",
      "/results/team/2017GPTR\n",
      "/results/team/2017GPTR10K\n",
      "/results/summary/2017GPTR10K\n",
      "None\n",
      "#tabs-1\n",
      "https://www.hubertiming.com/\n",
      "https://facebook.com/hubertiming/\n",
      "None\n",
      "[577, 443, \n",
      "\n",
      "                    LIBBY B MITCHELL\n",
      "\n",
      "                , F, HILLSBORO, 1:41:18, 1:42:10, ]\n",
      "['[Place, Bib, Name, Gender, City, Chip Time, Gun Time, Team]']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 583 entries, 0 to 581\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   [Place      583 non-null    object\n",
      " 1    Bib        581 non-null    object\n",
      " 2    Name       578 non-null    object\n",
      " 3    Gender     578 non-null    object\n",
      " 4    City       578 non-null    object\n",
      " 5    Chip Time  578 non-null    object\n",
      " 6    Gun Time   578 non-null    object\n",
      " 7    Team]      578 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 41.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 578 entries, 0 to 581\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   [Place      578 non-null    object\n",
      " 1    Bib        578 non-null    object\n",
      " 2    Name       578 non-null    object\n",
      " 3    Gender     578 non-null    object\n",
      " 4    City       578 non-null    object\n",
      " 5    Chip Time  578 non-null    object\n",
      " 6    Gun Time   578 non-null    object\n",
      " 7    Team]      578 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 40.6+ KB\n",
      "[' 36:21', ' 36:42', ' 37:44', ' 38:34', ' 39:21', ' 39:49', ' 40:04', ' 40:05', ' 40:17', ' 40:21', ' 40:28', ' 40:36', ' 40:43', ' 41:01', ' 41:19', ' 41:43', ' 41:59', ' 42:23', ' 42:29', ' 42:35', ' 42:36', ' 42:37', ' 42:56', ' 43:26', ' 43:27', ' 43:40', ' 43:42', ' 43:43', ' 43:46', ' 43:46', ' 43:47', ' 43:47', ' 44:04', ' 44:05', ' 44:06', ' 44:07', ' 44:23', ' 44:38', ' 44:41', ' 44:42', ' 44:48', ' 45:03', ' 45:05', ' 45:10', ' 45:11', ' 45:11', ' 45:13', ' 45:14', ' 45:15', ' 45:17', ' 45:27', ' 45:28', ' 45:29', ' 45:36', ' 45:36', ' 45:40', ' 45:40', ' 45:49', ' 45:55', ' 46:02', ' 46:13', ' 46:20', ' 46:23', ' 46:26', ' 46:33', ' 46:35', ' 46:41', ' 46:42', ' 46:45', ' 46:46', ' 46:47', ' 46:49', ' 46:49', ' 46:52', ' 46:55', ' 46:56', ' 47:00', ' 47:02', ' 47:05', ' 47:05', ' 47:08', ' 47:09', ' 47:27', ' 47:29', ' 47:31', ' 47:35', ' 47:42', ' 47:45', ' 47:49', ' 47:50', ' 47:55', ' 47:56', ' 47:56', ' 47:58', ' 47:59', ' 48:09', ' 48:13', ' 48:18', ' 48:18', ' 48:20', ' 48:21', ' 48:29', ' 48:30', ' 48:37', ' 48:38', ' 48:39', ' 48:40', ' 48:42', ' 48:43', ' 48:44', ' 48:49', ' 48:56', ' 49:03', ' 49:09', ' 49:12', ' 49:18', ' 49:23', ' 49:26', ' 49:30', ' 49:32', ' 49:35', ' 49:42', ' 49:44', ' 49:44', ' 49:51', ' 49:54', ' 49:58', ' 50:07', ' 50:12', ' 50:15', ' 50:18', ' 50:19', ' 50:22', ' 50:24', ' 50:26', ' 50:29', ' 50:36', ' 50:38', ' 50:39', ' 50:40', ' 50:41', ' 50:43', ' 50:46', ' 50:51', ' 51:00', ' 51:02', ' 51:05', ' 51:06', ' 51:08', ' 51:08', ' 51:10', ' 51:12', ' 51:18', ' 51:20', ' 51:22', ' 51:24', ' 51:34', ' 51:35', ' 51:38', ' 51:41', ' 51:43', ' 51:49', ' 51:52', ' 51:54', ' 52:15', ' 52:17', ' 52:25', ' 52:26', ' 52:34', ' 52:34', ' 52:35', ' 52:35', ' 52:36', ' 52:38', ' 52:38', ' 52:41', ' 52:43', ' 52:47', ' 52:48', ' 52:50', ' 52:55', ' 52:55', ' 52:58', ' 53:02', ' 53:05', ' 53:06', ' 53:09', ' 53:12', ' 53:16', ' 53:23', ' 53:26', ' 53:31', ' 53:32', ' 53:33', ' 53:37', ' 53:44', ' 53:45', ' 53:49', ' 53:53', ' 53:56', ' 53:58', ' 53:59', ' 53:59', ' 54:05', ' 54:09', ' 54:10', ' 54:16', ' 54:17', ' 54:17', ' 54:20', ' 54:21', ' 54:29', ' 54:38', ' 54:41', ' 54:41', ' 54:43', ' 54:44', ' 54:46', ' 54:50', ' 54:53', ' 55:00', ' 55:00', ' 55:00', ' 55:00', ' 55:00', ' 55:05', ' 55:12', ' 55:16', ' 55:19', ' 55:23', ' 55:28', ' 55:29', ' 55:33', ' 55:42', ' 55:45', ' 55:50', ' 55:56', ' 56:00', ' 56:06', ' 56:13', ' 56:19', ' 56:20', ' 56:26', ' 56:30', ' 56:31', ' 56:33', ' 56:38', ' 56:48', ' 56:55', ' 56:56', ' 56:58', ' 56:58', ' 57:03', ' 57:05', ' 57:05', ' 57:05', ' 57:13', ' 57:16', ' 57:17', ' 57:31', ' 57:31', ' 57:33', ' 57:33', ' 57:36', ' 57:38', ' 57:42', ' 57:43', ' 57:45', ' 57:51', ' 58:01', ' 58:02', ' 58:13', ' 58:15', ' 58:22', ' 58:28', ' 58:28', ' 58:30', ' 58:33', ' 58:35', ' 58:39', ' 58:40', ' 58:45', ' 58:47', ' 58:51', ' 58:51', ' 58:58', ' 58:59', ' 58:59', ' 59:01', ' 59:02', ' 59:11', ' 59:19', ' 59:27', ' 59:27', ' 59:29', ' 59:29', ' 59:30', ' 59:31', ' 59:32', ' 59:40', ' 59:41', ' 59:43', ' 59:47', ' 59:52', ' 59:53', ' 59:55', ' 1:00:01', ' 1:00:03', ' 1:00:12', ' 1:00:12', ' 1:00:13', ' 1:00:13', ' 1:00:18', ' 1:00:18', ' 1:00:18', ' 1:00:19', ' 1:00:20', ' 1:00:23', ' 1:00:24', ' 1:00:39', ' 1:00:41', ' 1:00:42', ' 1:00:47', ' 1:00:49', ' 1:00:56', ' 1:00:59', ' 1:01:01', ' 1:01:10', ' 1:01:18', ' 1:01:19', ' 1:01:21', ' 1:01:22', ' 1:01:22', ' 1:01:27', ' 1:01:30', ' 1:01:30', ' 1:01:32', ' 1:01:35', ' 1:01:48', ' 1:01:48', ' 1:01:48', ' 1:01:49', ' 1:02:04', ' 1:02:05', ' 1:02:05', ' 1:02:06', ' 1:02:10', ' 1:02:18', ' 1:02:21', ' 1:02:22', ' 1:02:25', ' 1:02:29', ' 1:02:30', ' 1:02:36', ' 1:02:39', ' 1:02:41', ' 1:02:43', ' 1:02:44', ' 1:02:47', ' 1:02:48', ' 1:02:48', ' 1:02:48', ' 1:03:02', ' 1:03:03', ' 1:03:08', ' 1:03:17', ' 1:03:19', ' 1:03:34', ' 1:03:34', ' 1:03:36', ' 1:03:44', ' 1:03:44', ' 1:03:49', ' 1:03:51', ' 1:03:51', ' 1:03:53', ' 1:04:07', ' 1:04:09', ' 1:04:09', ' 1:04:10', ' 1:04:11', ' 1:04:16', ' 1:04:19', ' 1:04:19', ' 1:04:22', ' 1:04:25', ' 1:04:28', ' 1:04:30', ' 1:04:33', ' 1:04:35', ' 1:04:37', ' 1:04:40', ' 1:04:44', ' 1:04:48', ' 1:04:51', ' 1:04:55', ' 1:04:56', ' 1:04:57', ' 1:04:59', ' 1:05:12', ' 1:05:13', ' 1:05:14', ' 1:05:16', ' 1:05:16', ' 1:05:16', ' 1:05:19', ' 1:05:21', ' 1:05:27', ' 1:05:33', ' 1:05:37', ' 1:05:44', ' 1:05:48', ' 1:05:53', ' 1:06:04', ' 1:06:14', ' 1:06:19', ' 1:06:28', ' 1:06:32', ' 1:06:38', ' 1:06:41', ' 1:06:43', ' 1:06:45', ' 1:06:46', ' 1:06:50', ' 1:06:51', ' 1:06:52', ' 1:06:53', ' 1:06:55', ' 1:07:04', ' 1:07:14', ' 1:07:14', ' 1:07:16', ' 1:07:16', ' 1:07:16', ' 1:07:18', ' 1:07:18', ' 1:07:23', ' 1:07:25', ' 1:07:28', ' 1:07:35', ' 1:07:41', ' 1:07:46', ' 1:07:51', ' 1:07:52', ' 1:07:56', ' 1:08:07', ' 1:08:11', ' 1:08:12', ' 1:08:16', ' 1:08:17', ' 1:08:18', ' 1:08:22', ' 1:08:33', ' 1:08:42', ' 1:08:47', ' 1:08:52', ' 1:08:54', ' 1:08:56', ' 1:08:57', ' 1:08:58', ' 1:09:04', ' 1:09:06', ' 1:09:06', ' 1:09:14', ' 1:09:15', ' 1:09:16', ' 1:09:18', ' 1:09:18', ' 1:09:19', ' 1:09:20', ' 1:09:27', ' 1:09:29', ' 1:09:31', ' 1:09:49', ' 1:09:59', ' 1:10:01', ' 1:10:04', ' 1:10:06', ' 1:10:13', ' 1:10:37', ' 1:10:50', ' 1:10:51', ' 1:10:55', ' 1:11:00', ' 1:11:01', ' 1:11:07', ' 1:11:09', ' 1:11:10', ' 1:11:10', ' 1:11:12', ' 1:11:21', ' 1:11:31', ' 1:11:39', ' 1:11:40', ' 1:11:54', ' 1:11:56', ' 1:11:59', ' 1:12:02', ' 1:12:08', ' 1:12:09', ' 1:12:14', ' 1:12:19', ' 1:12:20', ' 1:12:32', ' 1:12:42', ' 1:12:43', ' 1:12:53', ' 1:12:55', ' 1:13:02', ' 1:13:08', ' 1:13:23', ' 1:13:25', ' 1:13:30', ' 1:13:44', ' 1:13:49', ' 1:13:49', ' 1:14:01', ' 1:14:10', ' 1:14:13', ' 1:14:18', ' 1:14:19', ' 1:14:22', ' 1:14:32', ' 1:14:40', ' 1:14:54', ' 1:15:13', ' 1:15:34', ' 1:15:38', ' 1:15:48', ' 1:15:51', ' 1:15:58', ' 1:16:00', ' 1:16:19', ' 1:16:25', ' 1:16:46', ' 1:16:48', ' 1:16:52', ' 1:17:06', ' 1:17:14', ' 1:18:08', ' 1:18:09', ' 1:18:56', ' 1:20:46', ' 1:23:08', ' 1:23:09', ' 1:23:10', ' 1:23:12', ' 1:23:30', ' 1:23:30', ' 1:23:45', ' 1:23:54', ' 1:24:09', ' 1:24:19', ' 1:25:11', ' 1:25:49', ' 1:25:49', ' 1:26:22', ' 1:27:08', ' 1:28:19', ' 1:28:33', ' 1:29:04', ' 1:29:04', ' 1:31:20', ' 1:32:10', ' 1:32:18', ' 1:32:20', ' 1:32:28', ' 1:32:30', ' 1:33:50', ' 1:34:45', ' 1:34:48', ' 1:37:10', ' 1:38:17', ' 1:38:31', ' 1:38:32', ' 1:40:47', ' 1:41:18']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6476/3225456110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtime_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m     \u001b[0mmath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m60\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0mtime_mins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.request import urlopen\n",
    "import bs4\n",
    "# from bs4 import BeautifulSoup #for some reason this line is not working\n",
    "url = \"http://www.hubertiming.com/results/2017GPTR10K\"\n",
    "html = urlopen(url)\n",
    "\n",
    "soup = bs4.BeautifulSoup(html, 'lxml')\n",
    "type(soup)\n",
    "\n",
    "bs4.BeautifulSoup\n",
    "\n",
    "# Get the title\n",
    "title = soup.title\n",
    "# print(title)\n",
    "\n",
    "# print out the text\n",
    "text = soup.get_text()\n",
    "# print(soup.text)\n",
    "\n",
    "all_links = soup.find_all('a')\n",
    "for link in all_links:\n",
    "    print(link.get(\"href\"))\n",
    "\n",
    "# print out the first 10 rows for sanity check\n",
    "rows = soup.find_all('tr')\n",
    "# print(rows[:10])\n",
    "\n",
    "for row in rows:\n",
    "    row_td = row.find_all('td')\n",
    "# print(row_td)\n",
    "type(row_td)\n",
    "\n",
    "bs4.element.ResultSet\n",
    "\n",
    "# To remove html tags using Beautiful Soup, pass the string of interest into BeautifulSoup() \n",
    "# and use the get_text() method to extract the text without html tags\n",
    "\n",
    "str_cells = str(row_td)\n",
    "cleantext = bs4.BeautifulSoup(str_cells, 'lxml').get_text()\n",
    "# print(cleantext)\n",
    "\n",
    "#To gain a better understanding of the problem, try doing the same extraction with regular expressions.\n",
    "# Be sure to import the re (regular expressions) module in your python environment. The code below shows\n",
    "# how to build a regular expression that finds all the characters inside <td> html tags and replace them with\n",
    "# an empty string for each table row\n",
    "\n",
    "# First, the code compiles a regular expression by passing the regex to re.compile(). Compilation is not required,\n",
    "# but many python programming resources recommend compilation of regex to improve performance. This is esp the case \n",
    "# if the same regex is to be used over many lines of input text\n",
    "\n",
    "import re\n",
    "list_rows= []\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    str_cells = str(cells)\n",
    "    clean = re.compile('<.*?>')\n",
    "    clean2 = (re.sub(clean, '', str_cells))\n",
    "    list_rows.append(clean2)\n",
    "print(clean2)\n",
    "type(clean2)\n",
    "\n",
    "# The next step is to convert the list into a pandas dataframe and get a quick view of the first 10 rows of data.\n",
    "# Pandas is a wonderful tool for manipulating python data as tables. It's almost like having a database server or google sheets\n",
    "# within your python envir. We will use a couple of features in ths lab (and throughout the DataEng course), but I encourage you to\n",
    "# explore it much further, see the pandas wiki page for a start\n",
    "df = pd.DataFrame(list_rows)\n",
    "df.head(10)\n",
    "\n",
    "# Data Transformation\n",
    "# Our topic this week is \"Data Gathering\", and in a sense, you are done with the gathering. But to make things interesting you should continue \n",
    "# on and use the data that you gathered. Next we need to transform the data into something more meaningful. \n",
    "\n",
    "# The dataframe is not in the format we want. To clean it up, split the \"0\" column into multiple columns at the comma position. \n",
    "# Use the str.split() method\n",
    "\n",
    "df1 = df[0].str.split(',', expand=True)\n",
    "df1.head(10)\n",
    "\n",
    "# This looks much better, but there is still work to do. The dataframe has unwanted square brackets surrounding each row. Use the \n",
    "# strip() method to remove the opening square bracket on column \"0\"\n",
    "\n",
    "df1[0] = df1[0].str.strip('[')\n",
    "df1.head(10)\n",
    "\n",
    "# The table is missing table headers. So go back to BeautifulSoup and use its find_all() method to get the table headers\n",
    "col_labels = soup.find_all('th')\n",
    "all_header = []\n",
    "col_str = str(col_labels)\n",
    "cleantext2 = bs4.BeautifulSoup(col_str, \"lxml\").get_text()\n",
    "all_header.append(cleantext2)\n",
    "print(all_header)\n",
    "\n",
    "# Next convert the table headers to a new pandas dataframe\n",
    "\n",
    "df2 = pd.DataFrame(all_header)\n",
    "df2.head()\n",
    "\n",
    "# Again, split column \"0\" into multiple columns at the comma position for all rows\n",
    "df3 = df2[0].str.split(',', expand=True)\n",
    "df3.head()\n",
    "\n",
    "# Next, concatenate the two dataframes into one using the concat() method\n",
    "frames = [df3, df1]\n",
    "\n",
    "df4 = pd.concat(frames)\n",
    "df4.head(10)\n",
    "\n",
    "# Next, re-configure the data frame so that the first row is the table header\n",
    "df5 = df4.rename(columns=df4.iloc[0])\n",
    "df5.head()\n",
    "\n",
    "# Behold all of the progress that you have made! At this point, the table is almost properly formatted.\n",
    "# For analysis, start by getting an overview of the data as shown below\n",
    "df5.info()\n",
    "df5.shape\n",
    "\n",
    "# The table has 583 rows and 8 columns. One statement that you will hear me say over and over throughout\n",
    "# the course is, \"if you did not validate the data then it's probably wrong\", and this data is no exception\n",
    "# How do we know? Because it has 583 rows, but the various columns have varying numbers of \"non-null object\"\n",
    "# values. So transform it again to drop all rows with any missing values.\n",
    "\n",
    "df6 = df5.dropna(axis=0, how='any')\n",
    "df6.info()\n",
    "df6.shape\n",
    "\n",
    "# wasn't that easy? When I say \"pandas is useful\", this is 1 example of what I mean. YOu can do the same thing\n",
    "# of thing within a relational database, but by keeping all of the data in memory within our python program we can\n",
    "# accomplish this type of transformation task much more quickly and cleanly\n",
    "\n",
    "# In a real data engineering project probably we would validate the data thoroughly by devising predicates for every \n",
    "# aspect of the data. But to keep the focus on data gathering we will instead move forward with the data we have.\n",
    "\n",
    "# Notice how the table header is replicated as the first row in df5 and df6. Drop this redundant row like this:\n",
    "\n",
    "df7 = df6.drop(df6.index[0])\n",
    "df7.head()\n",
    "\n",
    "# Clean up the headers a bit more by renaming the '[Place' and 'Team]' columns. Python is picky about whitespace.\n",
    "# Make sure you include a space after the quotation mark in 'Team]'.\n",
    "df7.rename(columns={'[Place': 'Place'}, inplace=True)\n",
    "df7.rename(columns={' Team]': 'Team'}, inplace=True)\n",
    "df7.head()\n",
    "\n",
    "# The final data cleaning step involves removing the closing bracket for cells in the \"team\" column\n",
    "df7['Team'] = df7['Team'].str.strip(']')\n",
    "df7.head()\n",
    "\n",
    "# The dataframe is now in the desired format. One thing you could do at this point is to further transform the data\n",
    "# by eliminating redundancies and the unneeded information. For example, some of the columns appear to be redundant \n",
    "# and potentially could be dropped.\n",
    "\n",
    "# Next move on to the Data Science part, including computation of summary statistics and plotting of results.\n",
    "\n",
    "# DATA ANALYSIS AND VISUALIZATION\n",
    "\n",
    "# The first question to answer is, 'What was the average finish time (in minutes) for the runners?' unfortunately, the \"chip Time\"\n",
    "# field gives the finish time in hh:mm:ss, so you need to transform that column into just minutes. One way to do this is convert the column\n",
    "# to a list first for manipulation\n",
    "\n",
    "time_list = df7[' Chip Time'].tolist()\n",
    "\n",
    "# You can use a for-loop to convert 'Chip Time' to minutes\n",
    "\n",
    "time_mins = []\n",
    "print(time_list)\n",
    "for i in time_list:\n",
    "    m, s = i.split(':')\n",
    "    math = (int(m) * 60 + int(s))/60\n",
    "    time_mins.append(math)\n",
    "print(time_mins)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e866882f476c82f4f6555fd82842321de7c4e800835c783add7b7ac5c118fa1a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
